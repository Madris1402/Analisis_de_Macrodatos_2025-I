{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b59d9c-9652-4319-b31b-44dc52c94a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre = \"Madrigal Urencio Ricardo \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69855bcb-20e5-4e64-a6f2-46d0f0eba7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_hdfs = \"hdfs://namenode:9000/tmp/amd/locatellimpio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "514dc6cc-edad-4034-8c25-83fca25dff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madrigal Urencio Ricardo \n",
      "\n",
      "root\n",
      " |-- fecha_solicitud: date (nullable = true)\n",
      " |-- hora_solicitud: timestamp (nullable = true)\n",
      " |-- tema_solicitud: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- edad: double (nullable = true)\n",
      " |-- estatus: string (nullable = true)\n",
      " |-- alcaldia: string (nullable = true)\n",
      " |-- colonia_datos: string (nullable = true)\n",
      " |-- latitud: double (nullable = true)\n",
      " |-- longitud: double (nullable = true)\n",
      " |-- codigo_postal_solicitud: string (nullable = true)\n",
      " |-- d_codigo: integer (nullable = true)\n",
      " |-- D_mnpio: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(dir_hdfs, header = True, inferSchema = True)\n",
    "\n",
    "print(nombre)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7923ba39-50a7-4f67-b36b-65ad7ba92fd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshow tables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb98a509-db50-4eef-97b9-c34b16791139",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocatel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshow tables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"locatel\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b716adc-a40e-42f7-906b-0264b963a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nombre)\n",
    "spark.sql(\"describe locatel\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfd698-5c2b-4e4f-8508-6230c502c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select fecha_solicitud, edad, alcaldia from locatel\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff3863-531f-436d-b9b7-de785652a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlstr = \"\"\"select fecha_solicitud, edad, alcaldia\n",
    "from locatel\n",
    "where edad is not null\n",
    "\"\"\"\n",
    "spark.sql(sqlstr).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920623d-15ce-47fb-9d29-ce052a02b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlstr = \"\"\"select count(*) nr from locatel\"\"\"\n",
    "spark.sql(sqlstr).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893154f6-42f3-469f-837b-23c782d01666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Número de registros por sexo\n",
    "\n",
    "sqlstr = \"\"\"select sexo, count(*) nr \n",
    "from locatel\n",
    "group by sexo\n",
    "order by sexo\n",
    "\"\"\"\n",
    "spark.sql(sqlstr).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85506a44-5607-4be1-a30f-0c7579055e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Número de registros por edad\n",
    "campo = \"edad\"\n",
    "\n",
    "sqlstr = f\"\"\"select {campo}, count(*) nr \n",
    "from locatel\n",
    "group by {campo}\n",
    "order by {campo} desc\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "spark.sql(sqlstr).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e0342-c0d9-4d46-b315-ce268788cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Número de registros por tema\n",
    "campo = \"tema_solicitud\"\n",
    "\n",
    "sqlstr = f\"\"\"select {campo}, count(*) nr \n",
    "from locatel\n",
    "group by {campo}\n",
    "order by nr desc \n",
    "limit 5\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "spark.sql(sqlstr).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77e218-4419-41a3-9152-fe8ed61c8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edad por campo\n",
    "campo = \"tema_solicitud\"\n",
    "\n",
    "sqlstr = f\"\"\"select {campo}, round(avg(edad), 2) edad_prom,\n",
    "max(edad) edad_max, min(edad) edad_min, count(*) nr\n",
    "from locatel\n",
    "group by {campo}\n",
    "order by {campo}\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "spark.sql(sqlstr).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ae2db-82cc-4a99-9678-39a4da71bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edad por campo\n",
    "campo = \"sexo\"\n",
    "\n",
    "sqlstr = f\"\"\"select {campo}, round(avg(edad), 2) edad_prom,\n",
    "max(edad) edad_max, min(edad) edad_min, count(*) nr\n",
    "from locatel\n",
    "group by {campo}\n",
    "order by {campo}\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "spark.sql(sqlstr).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4057a-3eed-4a1c-903c-dee36e637a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de registros por hora\n",
    "campo = \"hora\"\n",
    "\n",
    "sqlstr = f\"\"\"select HOUR(hora_solicitud) hora, count(*) nr\n",
    "from locatel\n",
    "group by HOUR(hora_solicitud)\n",
    "order by 1\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "spark.sql(sqlstr).show(24, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2957f6-fdd0-4d8a-976b-c65e9706fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#Numero de registros por hora\n",
    "sqlstr = f\"\"\"\n",
    "    SELECT hora_solicitud hora, COUNT(*) nr\n",
    "    FROM locatel\n",
    "    GROUP BY hora_solicitud\n",
    "    ORDER BY 1\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "dfh = spark.sql(sqlstr).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "#plt.bar(dfh['hora'], dfh['nr'])\n",
    "plt.plot(dfh['hora'], dfh['nr'], marker='o', linestyle='-', color='r')\n",
    "\n",
    "plt.xlabel('Hora del dia')\n",
    "plt.ylabel('Numero de registros')\n",
    "plt.title('Numero de registros por hora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea7ba4-3c27-42cb-b446-60926b96f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT hora_solicitud FROM locatel\").show(10000, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51c10b-0358-4950-8535-b6c9c703c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Numero de registros por hora\n",
    "sqlstr = f\"\"\"\n",
    "    SELECT SUBSTRING(hora_solicitud,15,2) hora, COUNT(*) nr\n",
    "    FROM locatel\n",
    "    GROUP BY SUBSTRING(hora_solicitud,15,2)\n",
    "    ORDER BY 1\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "dfh = spark.sql(sqlstr).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(dfh['hora'], dfh['nr'])\n",
    "#plt.plot(dfh['hora'], dfh['nr'], marker='o', linestyle='-', color='r')\n",
    "\n",
    "plt.xlabel('HORA DEL DIA')\n",
    "plt.ylabel('NUMERO DE REGISTROS')\n",
    "plt.title('NUMERO DE REGISTROS POR HORA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b399a-0c4e-4269-92ac-0d14ae6a8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de registros por hora\n",
    "sqlstr = f\"\"\"\n",
    "    SELECT YEAR(fecha_solicitud) anio, COUNT(*) nr\n",
    "    FROM locatel\n",
    "    GROUP BY anio\n",
    "    ORDER BY 1\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "dfh = spark.sql(sqlstr).show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20e568-536c-465e-b723-056c4f63825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de registros por hora\n",
    "sqlstr = f\"\"\"\n",
    "    SELECT YEAR(fecha_solicitud) anio, COUNT(*) nr\n",
    "    FROM locatel\n",
    "    GROUP BY anio\n",
    "    ORDER BY 1\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "dfh = spark.sql(sqlstr).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(dfh['anio'], dfh['nr'])\n",
    "\n",
    "plt.xlabel('AÑO')\n",
    "plt.ylabel('NUMERO DE REGISTROS')\n",
    "plt.title('NUMERO DE REGISTROS POR AÑO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc24d3b-4ca1-4777-8956-804a6c05b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de registros por hora\n",
    "sqlstr = f\"\"\"\n",
    "    SELECT YEAR(fecha_solicitud) anio, sexo, COUNT(*) nr\n",
    "    FROM locatel\n",
    "    GROUP BY anio, sexo\n",
    "    ORDER BY 1\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "dfh = spark.sql(sqlstr).show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bed39d-3ed6-4df1-8ad4-b3b317967d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de registros por hora\n",
    "sqlstr = f\"\"\"\n",
    "    SELECT anio,\n",
    "    SUM(CASE WHEN sexo='FEMENINO' THEN nr ELSE 0 END) AS FEMENINO,\n",
    "    SUM(CASE WHEN sexo='MASCULINO' THEN nr ELSE 0 END) AS MASCULINO,\n",
    "    SUM(CASE WHEN sexo IN('NA', 'NO ESPECIFICADO') THEN nr ELSE 0 END) AS NO_ESPECIFICADO\n",
    "    FROM (    \n",
    "        SELECT YEAR(fecha_solicitud) anio, sexo, COUNT(*) nr\n",
    "        FROM locatel\n",
    "        GROUP BY anio, sexo\n",
    "    ) d\n",
    "    GROUP BY anio\n",
    "    ORDER BY 1\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "dfh = spark.sql(sqlstr).show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd52803-3754-4fca-b48f-18462f20384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de registros por hora\n",
    "sqlstr = f\"\"\"\n",
    "    SELECT anio,\n",
    "    SUM(CASE WHEN sexo='FEMENINO' THEN nr ELSE 0 END) AS FEMENINO,\n",
    "    SUM(CASE WHEN sexo='MASCULINO' THEN nr ELSE 0 END) AS MASCULINO,\n",
    "    SUM(CASE WHEN sexo IN('NA', 'NO ESPECIFICADO') THEN nr ELSE 0 END) AS NO_ESPECIFICADO\n",
    "    FROM (    \n",
    "        SELECT YEAR(fecha_solicitud) anio, sexo, COUNT(*) nr\n",
    "        FROM locatel\n",
    "        GROUP BY anio, sexo\n",
    "    ) d\n",
    "    GROUP BY anio\n",
    "    ORDER BY 1\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "dfh = spark.sql(sqlstr).toPandas()\\\n",
    "    .set_index('anio')[['FEMENINO', 'MASCULINO', 'NO_ESPECIFICADO']]\\\n",
    "    .plot(kind='bar', stacked=False, figsize=(10,6))\n",
    "\n",
    "plt.xlabel('AÑO')\n",
    "plt.ylabel('NUMERO DE REGISTROS')\n",
    "plt.title('NUMERO DE REGISTROS POR AÑO POR SEXO') \n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be199889-b422-46b8-a0ec-16a0b56f1cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de registros por hora\n",
    "sqlstr = f\"\"\"\n",
    "    SELECT anio,\n",
    "    SUM(CASE WHEN sexo='FEMENINO' THEN nr ELSE 0 END) AS FEMENINO,\n",
    "    SUM(CASE WHEN sexo='MASCULINO' THEN nr ELSE 0 END) AS MASCULINO,\n",
    "    SUM(CASE WHEN sexo IN('NA', 'NO ESPECIFICADO') THEN nr ELSE 0 END) AS NO_ESPECIFICADO\n",
    "    FROM (    \n",
    "        SELECT YEAR(fecha_solicitud) anio, sexo, COUNT(*) nr\n",
    "        FROM locatel\n",
    "        GROUP BY anio, sexo\n",
    "    ) d\n",
    "    GROUP BY anio\n",
    "    ORDER BY 1\n",
    "\"\"\"\n",
    "print(nombre)\n",
    "spark.sql(sqlstr)\\\n",
    "    .write.mode(\"append\")\\\n",
    "    .save(\"hdfs://namenode:9000/tmp/amd/locatelanio\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
