{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f87c478-023e-4879-b11e-a232a88c6192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ricardo Madrigal Urencio \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nombre = \"\\n Ricardo Madrigal Urencio \\n\"\n",
    "print(nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa47ae7-1f6b-4401-ac86-f5b904acc0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todas las librerias a Importar\n",
    "from pyspark.sql.functions import col, sum, to_date, to_timestamp, regexp_replace\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType\n",
    "from pyspark.sql.functions import hour, minute, second, year, month, dayofmonth, weekofyear\n",
    "from pyspark.sql.functions import monotonically_increasing_id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c206dc7b-d1b6-4e45-a97c-4cd7e7d4d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ricardo Madrigal Urencio \n",
      "\n",
      "root\n",
      " |-- Genero_Usuario: string (nullable = true)\n",
      " |-- Edad_Usuario: string (nullable = true)\n",
      " |-- Bici: string (nullable = true)\n",
      " |-- Ciclo_Estacion_Retiro: string (nullable = true)\n",
      " |-- Fecha_Retiro: string (nullable = true)\n",
      " |-- Hora_Retiro: string (nullable = true)\n",
      " |-- Ciclo_Estacion_Arribo: string (nullable = true)\n",
      " |-- Fecha_Arribo: string (nullable = true)\n",
      " |-- Hora_Arribo: string (nullable = true)\n",
      " |-- _c9: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Crear un solo Dataframe con todos los meses\n",
    "dir_hdfs = \"hdfs://namenode:9000/tmp/amd/ecobici/2019\"\n",
    "\n",
    "print(nombre)\n",
    "\n",
    "df = spark.read.csv(dir_hdfs, header=True, inferSchema=True)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df51bff-e5b1-4740-976d-f4b48e41001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ricardo Madrigal Urencio \n",
      "\n",
      "7718156\n"
     ]
    }
   ],
   "source": [
    "print(nombre)\n",
    "\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84eef01-3bd0-4caf-8a28-e1ae8c335330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ricardo Madrigal Urencio \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nombre)\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d86a8b-f6b8-4d2c-9c7d-19313be734ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nombre)\n",
    "\n",
    "df.filter(\"_c9 is not null\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c7b55-4b55-459a-9e9c-7d7bf1db5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar Registro Inválido\n",
    "\n",
    "print (nombre)\n",
    "print(df.count())\n",
    "\n",
    "\n",
    "df = df.filter(\"_c9 is null\")\n",
    "df = df.filter(col(\"Fecha_Retiro\").like('%2019'))\n",
    "\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12df28-1ac0-428c-aa83-ebe1c326d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nombre)\n",
    "\n",
    "df.orderBy(\"Fecha_Arribo\", ascending = True).show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd543c-c995-455d-8005-ebb927805f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza del esquema\n",
    "print(nombre)\n",
    "\n",
    "datos = df\\\n",
    ".withColumn(\"Edad_Usuario\", col(\"Edad_Usuario\").cast(FloatType())) \\\n",
    ".withColumn(\"Bici\", col(\"Bici\").cast(IntegerType())) \\\n",
    ".withColumn(\"Ciclo_Estacion_Retiro\", col(\"Ciclo_Estacion_Retiro\").cast(IntegerType())) \\\n",
    ".withColumn(\"Fecha_Retiro\", to_date(col(\"Fecha_Retiro\"), \"dd/MM/yyyy\")) \\\n",
    ".withColumn(\"Hora_Retiro\", date_format(col(\"Hora_Retiro\"), \"HH:mm:ss\")) \\\n",
    ".withColumn(\"Ciclo_Estacion_Arribo\", col(\"Ciclo_Estacion_Arribo\").cast(IntegerType())) \\\n",
    ".withColumn(\"Fecha_Arribo\", to_date(col(\"Fecha_Arribo\"), \"dd/MM/yyyy\")) \\\n",
    ".withColumn(\"Hora_Arribo\", date_format(col(\"Hora_Arribo\"), \"HH:mm:ss\"))\\\n",
    ".withColumn( \"id\", monotonically_increasing_id().cast(IntegerType()))\n",
    "datos.printSchema()\n",
    "datos.orderBy(\"Fecha_Arribo\", ascending = True).show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b246e-71c8-4289-84a6-b7f984dffbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=48>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o16.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "/usr/local/spark/python/pyspark/context.py:657: RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.\n",
      "  warnings.warn(\n",
      "Exception ignored in atexit callback: <function <lambda>.<locals>.<lambda> at 0x7fe71bc15300>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark-3.5.0-bin-hadoop3/python/pyspark/shell.py\", line 79, in <lambda>\n",
      "    atexit.register((lambda sc: lambda: sc.stop())(sc))\n",
      "                                        ^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 666, in stop\n",
      "    self._accumulatorServer.shutdown()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 316, in shutdown\n",
      "    SocketServer.TCPServer.shutdown(self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 253, in shutdown\n",
      "    self.__is_shut_down.wait()\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 629, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 327, in wait\n",
      "    waiter.acquire()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'sc'\n",
      "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/iostream.py\", line 580, in flush\n",
      "    if not evt.wait(self.flush_timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 629, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 331, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'sc'\n",
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x7fe765df7ec0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1359, in <lambda>\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'sc'\n"
     ]
    }
   ],
   "source": [
    "#Agregar campos año, mes, día, hora, minuto, segundo, día de la semana\n",
    "##Separar Fechas y Horas de Retiro en Campos Individuales\n",
    "\n",
    "from pyspark.sql.functions import hour, minute, second, year, month, dayofmonth, weekofyear\n",
    "colfs = \"Fecha_Retiro\"\n",
    "colhs = \"Hora_Retiro\"\n",
    "\n",
    "datos_tiempo_retiro = datos.select(\n",
    "    col(\"id\"), col(\"Genero_Usuario\"), col(\"Edad_Usuario\"), col(\"Bici\"), col(\"Ciclo_Estacion_Retiro\"),col(\"Fecha_Retiro\"),\n",
    "    date_format(col(colfs), \"EEEE\").alias(\"dia_sem_retiro\"),\n",
    "    dayofmonth(col(colfs)).alias(\"num_dia_retiro\"),\n",
    "    month(col(colfs)).alias(\"num_mes_retiro\"),\n",
    "    date_format(col(colfs), \"MMMM\").alias(\"mes_retiro\"),\n",
    "    year(col(colfs)).alias(\"anio_retiro\"),\n",
    "    col(\"Hora_Retiro\"),\n",
    "    hour(col(colhs)).alias(\"num_hora_retiro\"),\n",
    "    minute(col(colhs)).alias(\"minuto_retiro\"),    \n",
    "    second(col(colhs)).alias(\"segundo_retiro\")\n",
    "    \n",
    ")\n",
    "\n",
    "print(nombre)\n",
    "\n",
    "datos_tiempo_retiro.printSchema()\n",
    "\n",
    "datos_tiempo_retiro.orderBy('anio_retiro', ascending = True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298dd80-603c-43af-ad6b-e79ed908b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Separar Fechas y Horas de Arribo en Campos Individuales\n",
    "\n",
    "from pyspark.sql.functions import hour, minute, second, year, month, dayofmonth, weekofyear\n",
    "colfs = \"Fecha_Arribo\"\n",
    "colhs = \"Hora_Arribo\"\n",
    "\n",
    "datos_tiempo_arribo = datos.select(\n",
    "   col(\"id\"), col(\"Genero_Usuario\"), col(\"Edad_Usuario\"), col(\"Bici\"), col(\"Ciclo_Estacion_Arribo\"), col(\"Fecha_Arribo\"),\n",
    "    date_format(col(colfs), \"EEEE\").alias(\"dia_sem_arribo\"),\n",
    "    dayofmonth(col(colfs)).alias(\"num_dia_arribo\"),\n",
    "    month(col(colfs)).alias(\"num_mes_arribo\"),\n",
    "    date_format(col(colfs), \"MMMM\").alias(\"mes_arribo\"),\n",
    "    year(col(colfs)).alias(\"anio_arribo\"),\n",
    "    col(\"Hora_Arribo\"),\n",
    "    hour(col(colhs)).alias(\"num_hora_arribo\"),\n",
    "    minute(col(colhs)).alias(\"minuto_arribo\"),    \n",
    "    second(col(colhs)).alias(\"segundo_arribo\")\n",
    ")\n",
    "\n",
    "print(nombre)\n",
    "\n",
    "datos_tiempo_arribo.printSchema()\n",
    "\n",
    "datos_tiempo_arribo.orderBy('Fecha_Arribo', ascending = True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a4dea-0618-46f0-a467-383c4c643816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estadisticos Descriptivos de algunas columnas de cada DataFrame\n",
    "print(nombre)\n",
    "\n",
    "print(\"Estadisticas Descriptivas de Retiros\")\n",
    "columnas = [\"Genero_Usuario\", \"Edad_Usuario\", \"Bici\", \"Ciclo_Estacion_Retiro\", \"num_mes_retiro\", \"num_hora_retiro\", \"anio_retiro\", \"id\"]\n",
    "datos_tiempo_retiro.select(columnas).describe().show()\n",
    "\n",
    "print(\"Estadisticas Descriptivas de Arribos\")\n",
    "columnas = [\"Genero_Usuario\", \"Edad_Usuario\", \"Bici\", \"Ciclo_Estacion_Arribo\", \"num_mes_arribo\", \"num_hora_arribo\", \"anio_arribo\", \"id\"]\n",
    "datos_tiempo_arribo.select(columnas).describe().show()\n",
    "\n",
    "## Debido a Limitantes con mi equipo, tuve que reducir las columnas que hacen las estadísticas descriptivas ya que\n",
    "## se quedaba colgada con una carga de 100% de CPU y no entregaba ninguna salida. \n",
    "## (Lo dejé 10 minutos y no hizo nada, al reducirlas lo hizo en 2 mnutos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754dc57e-1c0d-4054-b9cc-6e7b03e53e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar el nuevo dataframe\n",
    "\n",
    "print(nombre)\n",
    "\n",
    "datos_tiempo_retiro.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"hdfs://namenode:9000/tmp/amd/ecobiciretiros\")\n",
    "print(\"Done\")\n",
    "\n",
    "datos_tiempo_arribo.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"hdfs://namenode:9000/tmp/amd/ecobiciarribos\")\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
